{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the ENC-DEC model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import dataloader\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import models\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision\n",
    "import os\n",
    "import time\n",
    "from torch_geometric.nn import GATv2Conv, global_mean_pool\n",
    "# reload library\n",
    "import importlib\n",
    "import cv2\n",
    "#import utils as ut\n",
    "import pandas as pd\n",
    "import DataDLC\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "import tqdm\n",
    "\n",
    "\n",
    "# PyTorch TensorBoard support\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'DataDLC' from 'c:\\\\Users\\\\Usuario\\\\Documents\\\\Documents\\\\MVA\\\\Stage\\\\DLCProject\\\\Code\\\\GitHubRep\\\\Behavioral_Tagging_of_Mice_in_multiple_Mice_dataset_using_Deep_Learning\\\\src\\\\DataDLC.py'>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(dataloader)\n",
    "importlib.reload(models)\n",
    "importlib.reload(DataDLC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This is only to create an eassy graph to test the build_graph_2 function**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    data_dlc.coords = data_dlc.coords.iloc[range(3), :]\n",
    "    data_dlc.n_frames = 3\n",
    "    data_dlc.save(r'C:\\Users\\Usuario\\Documents\\Documents\\MVA\\Stage\\DLCProject\\Data\\DataLoaderTestFormat\\output1.h5')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create dataset\n",
    "\n",
    "**Obs:** This is just a test to see if the model is working."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.26.4\n",
      "2.4.0+cpu\n"
     ]
    }
   ],
   "source": [
    "# print numpy version\n",
    "print(np.__version__)\n",
    "\n",
    "# print pytorch version\n",
    "print(torch.__version__)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'DataDLC' from 'c:\\\\Users\\\\Usuario\\\\Documents\\\\Documents\\\\MVA\\\\Stage\\\\DLCProject\\\\Code\\\\GitHubRep\\\\Behavioral_Tagging_of_Mice_in_multiple_Mice_dataset_using_Deep_Learning\\\\src\\\\DataDLC.py'>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(dataloader)\n",
    "importlib.reload(DataDLC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataloader.reload_module()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deactivate warnings\n",
    "if False:\n",
    "    import warnings\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "    data_loader = dataloader.DLCDataLoader(r'C:\\Users\\Usuario\\Documents\\Documents\\MVA\\Stage\\DLCProject\\Data\\DataLoaderTrainTest', batch_size=1, num_workers=0, device='cpu', window_size=3, stride = 1, build_graph=True)\n",
    "    #data_loader = dataloader.DLCDataLoader(r'C:\\Users\\Usuario\\Documents\\Documents\\MVA\\Stage\\DLCProject\\Data\\DataLoaderTestDataset', batch_size=1, num_workers=0, device='cpu', window_size=3, stride = 1, build_graph=True, behaviour='contacts generaux (R + V) active')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset\n",
    "\n",
    "*Graphs were already created and saved in the data folder*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "entire_dataset = torch.load(r'c:\\Users\\Usuario\\Documents\\Documents\\MVA\\Stage\\DLCProject\\Data\\DataLoader_Baseline_models\\dataset_w5-001.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset has 155042 samples\n"
     ]
    }
   ],
   "source": [
    "print('The dataset has %d samples' % len(entire_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entire_dataset[0].behaviour[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3,\n",
       "        4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2,\n",
       "        3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1,\n",
       "        2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0,\n",
       "        1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4,\n",
       "        0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3,\n",
       "        4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2,\n",
       "        3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4], dtype=torch.int32)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entire_dataset[-1].frame_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = entire_dataset.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only keep the 'General_Contact' behaviour.\n",
    "\n",
    "for i in range(len(dataset)):\n",
    "    dataset[i].y = torch.tensor([dataset[i].behaviour[1]], dtype=torch.long)\n",
    "    dataset[i].frame_mask = (dataset[i].frame_mask == 2).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1.,\n",
       "        0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "        0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "        1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0.,\n",
       "        0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1.,\n",
       "        0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "        0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "        1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0.])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entire_dataset[0].frame_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "180"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset[0].frame_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split Train-Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Shuffle**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suffle the dataset\n",
    "np.random.seed(0)\n",
    "np.random.shuffle(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train / Test split\n",
    "train_size = int(0.8 * len(dataset))\n",
    "\n",
    "train_dataset = dataset[:train_size]\n",
    "test_dataset = dataset[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The train dataset has 124033 samples\n",
      "The test dataset has 31009 samples\n"
     ]
    }
   ],
   "source": [
    "print('The train dataset has %d samples' % len(train_dataset))\n",
    "print('The test dataset has %d samples' % len(test_dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "\n",
    "# Create the dataloaders for train, validation and test\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, data in enumerate(train_loader):\n",
    "    first_batch = data\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataBatch(x=[2880, 4], edge_index=[2, 54240], file=[16], frame_mask=[2880], behaviour=[16], y=[16], batch=[2880], ptr=[17])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([36, 4])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_batch.x[first_batch.batch == 0][first_batch.frame_mask[first_batch.batch == 0] == 1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'models' from 'c:\\\\Users\\\\Usuario\\\\Documents\\\\Documents\\\\MVA\\\\Stage\\\\DLCProject\\\\Code\\\\GitHubRep\\\\Behavioral_Tagging_of_Mice_in_multiple_Mice_dataset_using_Deep_Learning\\\\src\\\\models.py'>"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "graphencoder = models.GATEncoder(nout = 64, nhid=32, attention_hidden=2, n_in=4, dropout=0.5)\n",
    "class_head = models.ClassificationHead(n_latent = 64, nhid=64, nout=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One try to see if the model is working\n",
    "embbed = graphencoder(first_batch.x, first_batch.edge_index, first_batch.frame_mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2880, 64])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embbed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "embbed = global_mean_pool(embbed, first_batch.batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 64])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embbed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = class_head(embbed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 2])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.GraphClassifier(graphencoder, class_head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model.forward(first_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 2])\n"
     ]
    }
   ],
   "source": [
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5493, 0.4507],\n",
       "        [0.5492, 0.4508],\n",
       "        [0.5496, 0.4504],\n",
       "        [0.5508, 0.4492],\n",
       "        [0.5522, 0.4478],\n",
       "        [0.5479, 0.4521],\n",
       "        [0.5489, 0.4511],\n",
       "        [0.5483, 0.4517],\n",
       "        [0.5443, 0.4557],\n",
       "        [0.5451, 0.4549],\n",
       "        [0.5489, 0.4511],\n",
       "        [0.5486, 0.4514],\n",
       "        [0.5501, 0.4499],\n",
       "        [0.5492, 0.4508],\n",
       "        [0.5470, 0.4530],\n",
       "        [0.5499, 0.4501]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7753\n"
     ]
    }
   ],
   "source": [
    "print(len(train_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_loader))\n",
    "\n",
    "x, edge_index, batch, frame_mask = batch.x, batch.edge_index, batch.batch, batch.frame_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 1.,  ..., 1., 0., 0.])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frame_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2880, 4])\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "tensors used as indices must be long, int, byte or bool tensors",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[118], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(x\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[43mframe_mask\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mshape)\n",
      "\u001b[1;31mIndexError\u001b[0m: tensors used as indices must be long, int, byte or bool tensors"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = models.SimpleMLPforGraph(n_in=144, n_hid=128, n_out=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainning loop\n",
    "num_epochs = 10\n",
    "lr = 0.001\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "writer = SummaryWriter(log_dir='runs/gcn_action_detection')  # TensorBoard writer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "124033it [56:13, 36.77it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Training Loss: 0.562334276328933, Training Accuracy: 0.7284029250280167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 31009/31009 [00:57<00:00, 537.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.5308780824410742, Validation Accuracy: 0.7653906930246057\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "124033it [1:32:53, 22.25it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Training Loss: 0.5243171418460332, Training Accuracy: 0.7758902872622608\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 31009/31009 [02:11<00:00, 236.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.5273605643061376, Validation Accuracy: 0.7726466509722983\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "124033it [23:55, 86.38it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Training Loss: 0.523211862823944, Training Accuracy: 0.7783170607822112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 31009/31009 [01:53<00:00, 273.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.5034721102385924, Validation Accuracy: 0.8010577574252636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "112948it [5:23:37, 122.29it/s]"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\n",
    "# Training Loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for i, data in tqdm.tqdm(enumerate(train_loader)):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data).unsqueeze(0)\n",
    "        labels = data.y\n",
    "        \n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "        train_loss += loss.item()\n",
    "        predicted = outputs.argmax(dim=1)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "        # Log training loss and accuracy at each step\n",
    "        if i % 10 == 0:  # Log every 10 iterations, adjust as needed\n",
    "            writer.add_scalar('Loss/Train', loss.item(), epoch * len(train_loader) + i)\n",
    "            writer.add_scalar('Accuracy/Train', correct / total, epoch * len(train_loader) + i)\n",
    "            #print(f\"Epoch {epoch+1}, Step {i}, Loss: {loss.item()}, Accuracy: {correct / total}\")\n",
    "\n",
    "    train_accuracy = correct / total\n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch+1}, Training Loss: {avg_train_loss}, Training Accuracy: {train_accuracy}\")\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for val_data in tqdm.tqdm(test_loader):\n",
    "            val_outputs = model(val_data).unsqueeze(0)\n",
    "            val_labels = val_data.y\n",
    "            val_loss += criterion(val_outputs, val_labels).item()\n",
    "            val_predicted = val_outputs.argmax(dim=1)\n",
    "            correct += (val_predicted == val_labels).sum().item()\n",
    "            total += val_labels.size(0)\n",
    "    \n",
    "    val_accuracy = correct / total\n",
    "    avg_val_loss = val_loss / len(test_loader)\n",
    "    \n",
    "    print(f\"Validation Loss: {avg_val_loss}, Validation Accuracy: {val_accuracy}\")\n",
    "\n",
    "    # Log validation metrics\n",
    "    writer.add_scalar('Loss/Validation', avg_val_loss, epoch)\n",
    "    writer.add_scalar('Accuracy/Validation', val_accuracy, epoch)\n",
    "\n",
    "# Close the TensorBoard writer\n",
    "writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    for i, data in tqdm.tqdm(enumerate(train_loader)):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data)\n",
    "        labels = data.y\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "        train_loss += loss.item()\n",
    "        predicted = outputs.argmax(dim=1)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "    \n",
    "        # Log training loss and accuracy at each step\n",
    "        if i % 10 == 0:  # Log every 10 iterations, adjust as needed\n",
    "            writer.add_scalar('Loss/Train', loss.item(), epoch * len(train_loader) + i)\n",
    "            writer.add_scalar('Accuracy/Train', correct / total, epoch * len(train_loader) + i)\n",
    "            #print(f\"Epoch {epoch+1}, Step {i}, Loss: {loss.item()}, Accuracy: {correct / total}\")\n",
    "\n",
    "    train_accuracy = correct / total\n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch+1}, Training Loss: {avg_train_loss}, Training Accuracy: {train_accuracy}\")\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for val_data in test_loader:\n",
    "            val_outputs = model(val_data)\n",
    "            val_labels = val_data.y\n",
    "            val_loss += criterion(val_outputs, val_labels).item()\n",
    "            val_predicted = val_outputs.argmax(dim=1)\n",
    "            correct += (val_predicted == val_labels).sum().item()\n",
    "            total += val_labels.size(0)\n",
    "    \n",
    "    val_accuracy = correct / total\n",
    "    avg_val_loss = val_loss / len(test_loader)\n",
    "    \n",
    "    print(f\"Validation Loss: {avg_val_loss}, Validation Accuracy: {val_accuracy}\")\n",
    "\n",
    "    # Log validation metrics\n",
    "    writer.add_scalar('Loss/Validation', avg_val_loss, epoch)\n",
    "    writer.add_scalar('Accuracy/Validation', val_accuracy, epoch)\n",
    "\n",
    "# Close the TensorBoard writer\n",
    "writer.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainning loop\n",
    "epochs = 50\n",
    "lr = 0.001\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    running_loss = 0.0\n",
    "    las\n",
    "    model.train()\n",
    "    for i, data in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        out = model.forward(data)\n",
    "        loss = criterion(out, data.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if i % 100\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = GraphAE.forward(data_loader.dataset[0].x, data_loader.dataset[0].edge_index, data_loader.dataset[0].frame_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and optimizer\n",
    "loss = GraphAE.loss(data_loader.dataset[0].x, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainning loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "encoder = models.GATEncoder(nout = 64, nhid=16, attention_hidden=2, n_in=3, dropout=0.5).to(device)\n",
    "print(encoder)\n",
    "decoder = models.GATDecoder(n_latent=64, n_hidden=16, n_out=3).to(device)\n",
    "print(decoder)\n",
    "model = models.GraphAE(encoder, decoder).to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "data = data_loader.dataset\n",
    "\n",
    "for epoch in range(100):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    for i in range(len(data)):\n",
    "\n",
    "        out = model(data[i].x.to(device), data[i].edge_index.to(device), data[i].frame_mask.to(device))\n",
    "        loss = model.loss(data[i].x, out)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f'Epoch {epoch}, Loss {loss.item()}')\n",
    "\n",
    "model.eval()\n",
    "out = model(data[0].x, data[0].edge_index, data[0].frame_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(out[0][0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numpyout = out[0][0].detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the output of the model, the first dimension are the points, and the second one is the x and y coordinates\n",
    "plt.scatter(numpyout[:,0], numpyout[:,1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_loader.dataset[0].x.shape)\n",
    "print(x[0].shape)\n",
    "print(x[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_loader.dataset[0].x.shape)\n",
    "print(x[0].shape)\n",
    "print(x[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contacts = behaviour.iloc[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "projenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
