{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading DLC 2.3.9...\n"
     ]
    }
   ],
   "source": [
    "import deeplabcut as dlc\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Project\n",
    "\n",
    "It's important to put the global path, not the relative one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "config_path = r'C:\\Users\\jalvarez\\Desktop\\mice_14bodypoints-Julian-2024-05-15 - Copie\\config.yaml'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visually check annotated frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    dlc.check_labels(\n",
    "    config_path,\n",
    "    draw_skeleton=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the training dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Drop the not annotated images**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31mSignature:\u001b[0m \u001b[0mdlc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropimagesduetolackofannotation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mDocstring:\u001b[0m\n",
      "Drop images from corresponding folder for not annotated images: /labeled-data/*folder*/CollectedData_*scorer*.h5\n",
      "Will be carried out iteratively for all *folders* in labeled-data.\n",
      "\n",
      "Parameter\n",
      "----------\n",
      "config : string\n",
      "    String containing the full path of the config file in the project.\n",
      "\u001b[1;31mFile:\u001b[0m      c:\\users\\jalvarez\\appdata\\local\\anaconda3\\envs\\deeplabcut\\lib\\site-packages\\deeplabcut\\generate_training_dataset\\trainingsetmanipulation.py\n",
      "\u001b[1;31mType:\u001b[0m      function"
     ]
    }
   ],
   "source": [
    "dlc.dropimagesduetolackofannotation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Annotated images:  20  In folder: 20\n",
      "PROCESSED: C:\\Users\\jalvarez\\Desktop\\mice_14bodypoints-Julian-2024-05-15 - Copie\\labeled-data\\Test 1  now # of annotated images:  20  in folder: 20\n",
      "Annotated images:  19  In folder: 19\n",
      "PROCESSED: C:\\Users\\jalvarez\\Desktop\\mice_14bodypoints-Julian-2024-05-15 - Copie\\labeled-data\\Test 10  now # of annotated images:  19  in folder: 19\n",
      "Annotated images:  16  In folder: 16\n",
      "PROCESSED: C:\\Users\\jalvarez\\Desktop\\mice_14bodypoints-Julian-2024-05-15 - Copie\\labeled-data\\Test 11  now # of annotated images:  16  in folder: 16\n",
      "Annotated images:  20  In folder: 20\n",
      "PROCESSED: C:\\Users\\jalvarez\\Desktop\\mice_14bodypoints-Julian-2024-05-15 - Copie\\labeled-data\\Test 18 (2)  now # of annotated images:  20  in folder: 20\n",
      "Annotated images:  20  In folder: 20\n",
      "PROCESSED: C:\\Users\\jalvarez\\Desktop\\mice_14bodypoints-Julian-2024-05-15 - Copie\\labeled-data\\Test 18  now # of annotated images:  20  in folder: 20\n",
      "Annotated images:  20  In folder: 20\n",
      "PROCESSED: C:\\Users\\jalvarez\\Desktop\\mice_14bodypoints-Julian-2024-05-15 - Copie\\labeled-data\\Test 19 (2)  now # of annotated images:  20  in folder: 20\n",
      "Annotated images:  20  In folder: 20\n",
      "PROCESSED: C:\\Users\\jalvarez\\Desktop\\mice_14bodypoints-Julian-2024-05-15 - Copie\\labeled-data\\Test 19 (3)  now # of annotated images:  20  in folder: 20\n",
      "Annotated images:  20  In folder: 20\n",
      "PROCESSED: C:\\Users\\jalvarez\\Desktop\\mice_14bodypoints-Julian-2024-05-15 - Copie\\labeled-data\\Test 19  now # of annotated images:  20  in folder: 20\n",
      "Annotated images:  19  In folder: 19\n",
      "PROCESSED: C:\\Users\\jalvarez\\Desktop\\mice_14bodypoints-Julian-2024-05-15 - Copie\\labeled-data\\Test 2  now # of annotated images:  19  in folder: 19\n",
      "Annotated images:  19  In folder: 19\n",
      "PROCESSED: C:\\Users\\jalvarez\\Desktop\\mice_14bodypoints-Julian-2024-05-15 - Copie\\labeled-data\\Test 20  now # of annotated images:  19  in folder: 19\n",
      "Annotated images:  20  In folder: 20\n",
      "PROCESSED: C:\\Users\\jalvarez\\Desktop\\mice_14bodypoints-Julian-2024-05-15 - Copie\\labeled-data\\Test 21  now # of annotated images:  20  in folder: 20\n",
      "Annotated images:  18  In folder: 18\n",
      "PROCESSED: C:\\Users\\jalvarez\\Desktop\\mice_14bodypoints-Julian-2024-05-15 - Copie\\labeled-data\\Test 25  now # of annotated images:  18  in folder: 18\n",
      "Annotated images:  20  In folder: 20\n",
      "PROCESSED: C:\\Users\\jalvarez\\Desktop\\mice_14bodypoints-Julian-2024-05-15 - Copie\\labeled-data\\Test 26  now # of annotated images:  20  in folder: 20\n",
      "Annotated images:  19  In folder: 19\n",
      "PROCESSED: C:\\Users\\jalvarez\\Desktop\\mice_14bodypoints-Julian-2024-05-15 - Copie\\labeled-data\\Test 27  now # of annotated images:  19  in folder: 19\n",
      "Annotated images:  18  In folder: 18\n",
      "PROCESSED: C:\\Users\\jalvarez\\Desktop\\mice_14bodypoints-Julian-2024-05-15 - Copie\\labeled-data\\Test 28 (2)  now # of annotated images:  18  in folder: 18\n",
      "Annotated images:  19  In folder: 19\n",
      "PROCESSED: C:\\Users\\jalvarez\\Desktop\\mice_14bodypoints-Julian-2024-05-15 - Copie\\labeled-data\\Test 28  now # of annotated images:  19  in folder: 19\n",
      "Annotated images:  20  In folder: 20\n",
      "PROCESSED: C:\\Users\\jalvarez\\Desktop\\mice_14bodypoints-Julian-2024-05-15 - Copie\\labeled-data\\Test 29  now # of annotated images:  20  in folder: 20\n",
      "Annotated images:  9  In folder: 9\n",
      "PROCESSED: C:\\Users\\jalvarez\\Desktop\\mice_14bodypoints-Julian-2024-05-15 - Copie\\labeled-data\\Test 30  now # of annotated images:  9  in folder: 9\n",
      "Annotated images:  20  In folder: 20\n",
      "PROCESSED: C:\\Users\\jalvarez\\Desktop\\mice_14bodypoints-Julian-2024-05-15 - Copie\\labeled-data\\Test 31  now # of annotated images:  20  in folder: 20\n",
      "Annotated images:  16  In folder: 16\n",
      "PROCESSED: C:\\Users\\jalvarez\\Desktop\\mice_14bodypoints-Julian-2024-05-15 - Copie\\labeled-data\\Test 36  now # of annotated images:  16  in folder: 16\n",
      "Annotated images:  7  In folder: 7\n",
      "PROCESSED: C:\\Users\\jalvarez\\Desktop\\mice_14bodypoints-Julian-2024-05-15 - Copie\\labeled-data\\Test 41  now # of annotated images:  7  in folder: 7\n",
      "Annotated images:  19  In folder: 19\n",
      "PROCESSED: C:\\Users\\jalvarez\\Desktop\\mice_14bodypoints-Julian-2024-05-15 - Copie\\labeled-data\\Test 45  now # of annotated images:  19  in folder: 19\n",
      "Annotated images:  14  In folder: 14\n",
      "PROCESSED: C:\\Users\\jalvarez\\Desktop\\mice_14bodypoints-Julian-2024-05-15 - Copie\\labeled-data\\Test 48  now # of annotated images:  14  in folder: 14\n",
      "Annotated images:  20  In folder: 20\n",
      "PROCESSED: C:\\Users\\jalvarez\\Desktop\\mice_14bodypoints-Julian-2024-05-15 - Copie\\labeled-data\\Test 5  now # of annotated images:  20  in folder: 20\n",
      "Annotated images:  19  In folder: 19\n",
      "PROCESSED: C:\\Users\\jalvarez\\Desktop\\mice_14bodypoints-Julian-2024-05-15 - Copie\\labeled-data\\Test 59  now # of annotated images:  19  in folder: 19\n",
      "Annotated images:  20  In folder: 20\n",
      "PROCESSED: C:\\Users\\jalvarez\\Desktop\\mice_14bodypoints-Julian-2024-05-15 - Copie\\labeled-data\\Test 6  now # of annotated images:  20  in folder: 20\n",
      "Annotated images:  20  In folder: 20\n",
      "PROCESSED: C:\\Users\\jalvarez\\Desktop\\mice_14bodypoints-Julian-2024-05-15 - Copie\\labeled-data\\Test 69  now # of annotated images:  20  in folder: 20\n",
      "Annotated images:  1  In folder: 1\n",
      "PROCESSED: C:\\Users\\jalvarez\\Desktop\\mice_14bodypoints-Julian-2024-05-15 - Copie\\labeled-data\\Test 74  now # of annotated images:  1  in folder: 1\n",
      "Attention: C:\\Users\\jalvarez\\Desktop\\mice_14bodypoints-Julian-2024-05-15 - Copie\\labeled-data\\Test 75 does not appear to have labeled data!\n",
      "Annotated images:  20  In folder: 20\n",
      "PROCESSED: C:\\Users\\jalvarez\\Desktop\\mice_14bodypoints-Julian-2024-05-15 - Copie\\labeled-data\\Test 8  now # of annotated images:  20  in folder: 20\n",
      "Annotated images:  20  In folder: 20\n",
      "PROCESSED: C:\\Users\\jalvarez\\Desktop\\mice_14bodypoints-Julian-2024-05-15 - Copie\\labeled-data\\Test 9  now # of annotated images:  20  in folder: 20\n",
      "Annotated images:  19  In folder: 19\n",
      "PROCESSED: C:\\Users\\jalvarez\\Desktop\\mice_14bodypoints-Julian-2024-05-15 - Copie\\labeled-data\\Test 1 - 354  now # of annotated images:  19  in folder: 19\n",
      "Annotated images:  8  In folder: 8\n",
      "PROCESSED: C:\\Users\\jalvarez\\Desktop\\mice_14bodypoints-Julian-2024-05-15 - Copie\\labeled-data\\Test 2 - 355  now # of annotated images:  8  in folder: 8\n",
      "Annotated images:  20  In folder: 20\n",
      "PROCESSED: C:\\Users\\jalvarez\\Desktop\\mice_14bodypoints-Julian-2024-05-15 - Copie\\labeled-data\\Test 4 - 357  now # of annotated images:  20  in folder: 20\n",
      "Annotated images:  18  In folder: 18\n",
      "PROCESSED: C:\\Users\\jalvarez\\Desktop\\mice_14bodypoints-Julian-2024-05-15 - Copie\\labeled-data\\Test 6 - 359  now # of annotated images:  18  in folder: 18\n",
      "Attention: C:\\Users\\jalvarez\\Desktop\\mice_14bodypoints-Julian-2024-05-15 - Copie\\labeled-data\\Test 8 - 361 does not appear to have labeled data!\n"
     ]
    }
   ],
   "source": [
    "dlc.dropimagesduetolackofannotation(config_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31mSignature:\u001b[0m\n",
      "\u001b[0mdlc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate_multianimaltraining_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mconfig\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mnum_shuffles\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mShuffles\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mwindows2linux\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mnet_type\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mnumdigits\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mcrop_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m400\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m400\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mcrop_sampling\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'hybrid'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mpaf_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mtrainIndices\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mtestIndices\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mn_edges_threshold\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m105\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mpaf_graph_degree\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mDocstring:\u001b[0m\n",
      "Creates a training dataset for multi-animal datasets. Labels from all the extracted frames are merged into a single .h5 file.\n",
      "\n",
      "Only the videos included in the config file are used to create this dataset.\n",
      "\n",
      "[OPTIONAL] Use the function 'add_new_videos' at any stage of the project to add more videos to the project.\n",
      "\n",
      "Important differences to standard:\n",
      " - stores coordinates with numdigits as many digits\n",
      " - creates\n",
      "Parameter\n",
      "----------\n",
      "config : string\n",
      "    Full path of the config.yaml file as a string.\n",
      "\n",
      "num_shuffles : int, optional\n",
      "    Number of shuffles of training dataset to create, i.e. [1,2,3] for num_shuffles=3. Default is set to 1.\n",
      "\n",
      "Shuffles: list of shuffles.\n",
      "    Alternatively the user can also give a list of shuffles (integers!).\n",
      "\n",
      "net_type: string\n",
      "    Type of networks. Currently resnet_50, resnet_101, and resnet_152, efficientnet-b0, efficientnet-b1, efficientnet-b2, efficientnet-b3,\n",
      "    efficientnet-b4, efficientnet-b5, and efficientnet-b6 as well as dlcrnet_ms5 are supported (not the MobileNets!).\n",
      "    See Lauer et al. 2021 https://www.biorxiv.org/content/10.1101/2021.04.30.442096v1\n",
      "\n",
      "numdigits: int, optional\n",
      "\n",
      "crop_size: tuple of int, optional\n",
      "    Dimensions (width, height) of the crops for data augmentation.\n",
      "    Default is 400x400.\n",
      "\n",
      "crop_sampling: str, optional\n",
      "    Crop centers sampling method. Must be either:\n",
      "    \"uniform\" (randomly over the image),\n",
      "    \"keypoints\" (randomly over the annotated keypoints),\n",
      "    \"density\" (weighing preferentially dense regions of keypoints),\n",
      "    or \"hybrid\" (alternating randomly between \"uniform\" and \"density\").\n",
      "    Default is \"hybrid\".\n",
      "\n",
      "paf_graph: list of lists, or \"config\" optional (default=None)\n",
      "    If not None, overwrite the default complete graph. This is useful for advanced users who\n",
      "    already know a good graph, or simply want to use a specific one. Note that, in that case,\n",
      "    the data-driven selection procedure upon model evaluation will be skipped.\n",
      "\n",
      "    \"config\" will use the skeleton defined in the config file.\n",
      "\n",
      "trainIndices: list of lists, optional (default=None)\n",
      "    List of one or multiple lists containing train indexes.\n",
      "    A list containing two lists of training indexes will produce two splits.\n",
      "\n",
      "testIndices: list of lists, optional (default=None)\n",
      "    List of one or multiple lists containing test indexes.\n",
      "\n",
      "n_edges_threshold: int, optional (default=105)\n",
      "    Number of edges above which the graph is automatically pruned.\n",
      "\n",
      "paf_graph_degree: int, optional (default=6)\n",
      "    Degree of paf_graph when automatically pruning it (before training).\n",
      "\n",
      "Example\n",
      "--------\n",
      ">>> deeplabcut.create_multianimaltraining_dataset('/analysis/project/reaching-task/config.yaml',num_shuffles=1)\n",
      "\n",
      ">>> deeplabcut.create_multianimaltraining_dataset('/analysis/project/reaching-task/config.yaml', Shuffles=[0,1,2], trainIndices=[trainInd1, trainInd2, trainInd3], testIndices=[testInd1, testInd2, testInd3])\n",
      "\n",
      "Windows:\n",
      ">>> deeplabcut.create_multianimaltraining_dataset(r'C:\\Users\\Ulf\\looming-task\\config.yaml',Shuffles=[3,17,5])\n",
      "--------\n",
      "\u001b[1;31mFile:\u001b[0m      c:\\users\\jalvarez\\appdata\\local\\anaconda3\\envs\\deeplabcut\\lib\\site-packages\\deeplabcut\\generate_training_dataset\\multiple_individuals_trainingsetmanipulation.py\n",
      "\u001b[1;31mType:\u001b[0m      function"
     ]
    }
   ],
   "source": [
    "dlc.create_multianimaltraining_dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jalvarez\\Desktop\\mice_14bodypoints-Julian-2024-05-15 - Copie\\labeled-data\\Test 75\\CollectedData_Julian.h5  not found (perhaps not annotated).\n",
      "C:\\Users\\jalvarez\\Desktop\\mice_14bodypoints-Julian-2024-05-15 - Copie\\labeled-data\\Test 8 - 361\\CollectedData_Julian.h5  not found (perhaps not annotated).\n",
      "Utilizing the following graph: [[0, 1], [0, 2], [0, 3], [0, 4], [0, 5], [0, 6], [0, 7], [0, 8], [0, 9], [0, 10], [0, 11], [0, 12], [0, 13], [1, 2], [1, 3], [1, 4], [1, 5], [1, 6], [1, 7], [1, 8], [1, 9], [1, 10], [1, 11], [1, 12], [1, 13], [2, 3], [2, 4], [2, 5], [2, 6], [2, 7], [2, 8], [2, 9], [2, 10], [2, 11], [2, 12], [2, 13], [3, 4], [3, 5], [3, 6], [3, 7], [3, 8], [3, 9], [3, 10], [3, 11], [3, 12], [3, 13], [4, 5], [4, 6], [4, 7], [4, 8], [4, 9], [4, 10], [4, 11], [4, 12], [4, 13], [5, 6], [5, 7], [5, 8], [5, 9], [5, 10], [5, 11], [5, 12], [5, 13], [6, 7], [6, 8], [6, 9], [6, 10], [6, 11], [6, 12], [6, 13], [7, 8], [7, 9], [7, 10], [7, 11], [7, 12], [7, 13], [8, 9], [8, 10], [8, 11], [8, 12], [8, 13], [9, 10], [9, 11], [9, 12], [9, 13], [10, 11], [10, 12], [10, 13], [11, 12], [11, 13], [12, 13]]\n",
      "Creating training data for: Shuffle: 1 TrainFraction:  0.95\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 567/567 [00:00<00:00, 1319.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training dataset is successfully created. Use the function 'train_network' to start training. Happy training!\n"
     ]
    }
   ],
   "source": [
    "dlc.create_multianimaltraining_dataset(\n",
    "    config_path,\n",
    "    num_shuffles=1,\n",
    "    net_type=\"dlcrnet_ms5\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31mSignature:\u001b[0m\n",
      "\u001b[0mdlc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_network\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mconfig\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mtrainingsetindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mmax_snapshots_to_keep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mdisplayiters\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0msaveiters\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mmaxiters\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mallow_growth\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mgputouse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mautotune\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mkeepdeconvweights\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mmodelprefix\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0msuperanimal_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0msuperanimal_transfer_learning\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mDocstring:\u001b[0m\n",
      "Trains the network with the labels in the training dataset.\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    config : string\n",
      "        Full path of the config.yaml file as a string.\n",
      "\n",
      "    shuffle: int, optional, default=1\n",
      "        Integer value specifying the shuffle index to select for training.\n",
      "\n",
      "    trainingsetindex: int, optional, default=0\n",
      "        Integer specifying which TrainingsetFraction to use.\n",
      "        Note that TrainingFraction is a list in config.yaml.\n",
      "\n",
      "    max_snapshots_to_keep: int or None\n",
      "        Sets how many snapshots are kept, i.e. states of the trained network. Every\n",
      "        saving iteration many times a snapshot is stored, however only the last\n",
      "        ``max_snapshots_to_keep`` many are kept! If you change this to None, then all\n",
      "        are kept.\n",
      "        See: https://github.com/DeepLabCut/DeepLabCut/issues/8#issuecomment-387404835\n",
      "\n",
      "    displayiters: optional, default=None\n",
      "        This variable is actually set in ``pose_config.yaml``. However, you can\n",
      "        overwrite it with this hack. Don't use this regularly, just if you are too lazy\n",
      "        to dig out the ``pose_config.yaml`` file for the corresponding project. If\n",
      "        ``None``, the value from there is used, otherwise it is overwritten!\n",
      "\n",
      "    saveiters: optional, default=None\n",
      "        This variable is actually set in ``pose_config.yaml``. However, you can\n",
      "        overwrite it with this hack. Don't use this regularly, just if you are too lazy\n",
      "        to dig out the ``pose_config.yaml`` file for the corresponding project.\n",
      "        If ``None``, the value from there is used, otherwise it is overwritten!\n",
      "\n",
      "    maxiters: optional, default=None\n",
      "        This variable is actually set in ``pose_config.yaml``. However, you can\n",
      "        overwrite it with this hack. Don't use this regularly, just if you are too lazy\n",
      "        to dig out the ``pose_config.yaml`` file for the corresponding project.\n",
      "        If ``None``, the value from there is used, otherwise it is overwritten!\n",
      "\n",
      "    allow_growth: bool, optional, default=True.\n",
      "        For some smaller GPUs the memory issues happen. If ``True``, the memory\n",
      "        allocator does not pre-allocate the entire specified GPU memory region, instead\n",
      "        starting small and growing as needed.\n",
      "        See issue: https://forum.image.sc/t/how-to-stop-running-out-of-vram/30551/2\n",
      "\n",
      "    gputouse: optional, default=None\n",
      "        Natural number indicating the number of your GPU (see number in nvidia-smi).\n",
      "        If you do not have a GPU put None.\n",
      "        See: https://nvidia.custhelp.com/app/answers/detail/a_id/3751/~/useful-nvidia-smi-queries\n",
      "\n",
      "    autotune: bool, optional, default=False\n",
      "        Property of TensorFlow, somehow faster if ``False``\n",
      "        (as Eldar found out, see https://github.com/tensorflow/tensorflow/issues/13317).\n",
      "\n",
      "    keepdeconvweights: bool, optional, default=True\n",
      "        Also restores the weights of the deconvolution layers (and the backbone) when\n",
      "        training from a snapshot. Note that if you change the number of bodyparts, you\n",
      "        need to set this to false for re-training.\n",
      "\n",
      "    modelprefix: str, optional, default=\"\"\n",
      "        Directory containing the deeplabcut models to use when evaluating the network.\n",
      "        By default, the models are assumed to exist in the project folder.\n",
      "\n",
      "    superanimal_name: str, optional, default =\"\"\n",
      "        Specified if transfer learning with superanimal is desired\n",
      "\n",
      "    superanimal_transfer_learning: bool, optional, default = False.\n",
      "        If set true, the training is transfer learning (new decoding layer). If set false,\n",
      "and superanimal_name is True, then the training is fine-tuning (reusing the decoding layer)\n",
      "\n",
      "    Returns\n",
      "    -------\n",
      "    None\n",
      "\n",
      "    Examples\n",
      "    --------\n",
      "    To train the network for first shuffle of the training dataset\n",
      "\n",
      "    >>> deeplabcut.train_network('/analysis/project/reaching-task/config.yaml')\n",
      "\n",
      "    To train the network for second shuffle of the training dataset\n",
      "\n",
      "    >>> deeplabcut.train_network(\n",
      "            '/analysis/project/reaching-task/config.yaml',\n",
      "            shuffle=2,\n",
      "            keepdeconvweights=True,\n",
      "        )\n",
      "\u001b[1;31mFile:\u001b[0m      c:\\users\\jalvarez\\appdata\\local\\anaconda3\\envs\\deeplabcut\\lib\\site-packages\\deeplabcut\\pose_estimation_tensorflow\\training.py\n",
      "\u001b[1;31mType:\u001b[0m      function"
     ]
    }
   ],
   "source": [
    "dlc.train_network?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    deeplabcut.train_network(\n",
    "    config_path,\n",
    "    saveiters=10000,\n",
    "    maxiters=50000,\n",
    "    allow_growth=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    deeplabcut.evaluate_network(\n",
    "    config_path,\n",
    "    plotting=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze a video (extracts detections and association costs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31mSignature:\u001b[0m\n",
      "\u001b[0mdlc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0manalyze_videos\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mconfig\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mvideos\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mvideotype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mtrainingsetindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mgputouse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0msave_as_csv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0min_random_order\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mdestfolder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mbatchsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mcropping\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mTFGPUinference\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mdynamic\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mmodelprefix\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mrobust_nframes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mallow_growth\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0muse_shelve\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mauto_track\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mn_tracks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mcalibrate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0midentity_only\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0muse_openvino\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mDocstring:\u001b[0m\n",
      "Makes prediction based on a trained network.\n",
      "\n",
      "The index of the trained network is specified by parameters in the config file\n",
      "(in particular the variable 'snapshotindex').\n",
      "\n",
      "The labels are stored as MultiIndex Pandas Array, which contains the name of\n",
      "the network, body part name, (x, y) label position in pixels, and the\n",
      "likelihood for each frame per body part. These arrays are stored in an\n",
      "efficient Hierarchical Data Format (HDF) in the same directory where the video\n",
      "is stored. However, if the flag save_as_csv is set to True, the data can also\n",
      "be exported in comma-separated values format (.csv), which in turn can be\n",
      "imported in many programs, such as MATLAB, R, Prism, etc.\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "config: str\n",
      "    Full path of the config.yaml file.\n",
      "\n",
      "videos: list[str]\n",
      "    A list of strings containing the full paths to videos for analysis or a path to\n",
      "    the directory, where all the videos with same extension are stored.\n",
      "\n",
      "videotype: str, optional, default=\"\"\n",
      "    Checks for the extension of the video in case the input to the video is a\n",
      "    directory. Only videos with this extension are analyzed. If left unspecified,\n",
      "    videos with common extensions ('avi', 'mp4', 'mov', 'mpeg', 'mkv') are kept.\n",
      "\n",
      "shuffle: int, optional, default=1\n",
      "    An integer specifying the shuffle index of the training dataset used for\n",
      "    training the network.\n",
      "\n",
      "trainingsetindex: int, optional, default=0\n",
      "    Integer specifying which TrainingsetFraction to use.\n",
      "    By default the first (note that TrainingFraction is a list in config.yaml).\n",
      "\n",
      "gputouse: int or None, optional, default=None\n",
      "    Indicates the GPU to use (see number in ``nvidia-smi``). If you do not have a\n",
      "    GPU put ``None``.\n",
      "    See: https://nvidia.custhelp.com/app/answers/detail/a_id/3751/~/useful-nvidia-smi-queries\n",
      "\n",
      "save_as_csv: bool, optional, default=False\n",
      "    Saves the predictions in a .csv file.\n",
      "\n",
      "in_random_order: bool, optional (default=True)\n",
      "    Whether or not to analyze videos in a random order.\n",
      "    This is only relevant when specifying a video directory in `videos`.\n",
      "\n",
      "destfolder: string or None, optional, default=None\n",
      "    Specifies the destination folder for analysis data. If ``None``, the path of\n",
      "    the video is used. Note that for subsequent analysis this folder also needs to\n",
      "    be passed.\n",
      "\n",
      "batchsize: int or None, optional, default=None\n",
      "    Change batch size for inference; if given overwrites value in ``pose_cfg.yaml``.\n",
      "\n",
      "cropping: list or None, optional, default=None\n",
      "    List of cropping coordinates as [x1, x2, y1, y2].\n",
      "    Note that the same cropping parameters will then be used for all videos.\n",
      "    If different video crops are desired, run ``analyze_videos`` on individual\n",
      "    videos with the corresponding cropping coordinates.\n",
      "\n",
      "TFGPUinference: bool, optional, default=True\n",
      "    Perform inference on GPU with TensorFlow code. Introduced in \"Pretraining\n",
      "    boosts out-of-domain robustness for pose estimation\" by Alexander Mathis,\n",
      "    Mert Yüksekgönül, Byron Rogers, Matthias Bethge, Mackenzie W. Mathis.\n",
      "    Source: https://arxiv.org/abs/1909.11229\n",
      "\n",
      "dynamic: tuple(bool, float, int) triple containing (state, detectiontreshold, margin)\n",
      "    If the state is true, then dynamic cropping will be performed. That means that if an object is detected (i.e. any body part > detectiontreshold),\n",
      "    then object boundaries are computed according to the smallest/largest x position and smallest/largest y position of all body parts. This  window is\n",
      "    expanded by the margin and from then on only the posture within this crop is analyzed (until the object is lost, i.e. <detectiontreshold). The\n",
      "    current position is utilized for updating the crop window for the next frame (this is why the margin is important and should be set large\n",
      "    enough given the movement of the animal).\n",
      "\n",
      "modelprefix: str, optional, default=\"\"\n",
      "    Directory containing the deeplabcut models to use when evaluating the network.\n",
      "    By default, the models are assumed to exist in the project folder.\n",
      "\n",
      "robust_nframes: bool, optional, default=False\n",
      "    Evaluate a video's number of frames in a robust manner.\n",
      "    This option is slower (as the whole video is read frame-by-frame),\n",
      "    but does not rely on metadata, hence its robustness against file corruption.\n",
      "\n",
      "allow_growth: bool, optional, default=False.\n",
      "    For some smaller GPUs the memory issues happen. If ``True``, the memory\n",
      "    allocator does not pre-allocate the entire specified GPU memory region, instead\n",
      "    starting small and growing as needed.\n",
      "    See issue: https://forum.image.sc/t/how-to-stop-running-out-of-vram/30551/2\n",
      "\n",
      "use_shelve: bool, optional, default=False\n",
      "    By default, data are dumped in a pickle file at the end of the video analysis.\n",
      "    Otherwise, data are written to disk on the fly using a \"shelf\"; i.e., a\n",
      "    pickle-based, persistent, database-like object by default, resulting in\n",
      "    constant memory footprint.\n",
      "\n",
      "The following parameters are only relevant for multi-animal projects:\n",
      "\n",
      "auto_track: bool, optional, default=True\n",
      "    By default, tracking and stitching are automatically performed, producing the\n",
      "    final h5 data file. This is equivalent to the behavior for single-animal\n",
      "    projects.\n",
      "\n",
      "    If ``False``, one must run ``convert_detections2tracklets`` and\n",
      "    ``stitch_tracklets`` afterwards, in order to obtain the h5 file.\n",
      "\n",
      "This function has 3 related sub-calls:\n",
      "\n",
      "identity_only: bool, optional, default=False\n",
      "    If ``True`` and animal identity was learned by the model, assembly and tracking\n",
      "    rely exclusively on identity prediction.\n",
      "\n",
      "calibrate: bool, optional, default=False\n",
      "    If ``True``, use training data to calibrate the animal assembly procedure. This\n",
      "    improves its robustness to wrong body part links, but requires very little\n",
      "    missing data.\n",
      "\n",
      "n_tracks: int or None, optional, default=None\n",
      "    Number of tracks to reconstruct. By default, taken as the number of individuals\n",
      "    defined in the config.yaml. Another number can be passed if the number of\n",
      "    animals in the video is different from the number of animals the model was\n",
      "    trained on.\n",
      "\n",
      "use_openvino: str, optional\n",
      "    Use \"CPU\" for inference if OpenVINO is available in the Python environment.\n",
      "\n",
      "Returns\n",
      "-------\n",
      "DLCScorer: str\n",
      "    the scorer used to analyze the videos\n",
      "\n",
      "Examples\n",
      "--------\n",
      "\n",
      "Analyzing a single video on Windows\n",
      "\n",
      ">>> deeplabcut.analyze_videos(\n",
      "        'C:\\myproject\\reaching-task\\config.yaml',\n",
      "        ['C:\\yourusername\\rig-95\\Videos\\reachingvideo1.avi'],\n",
      "    )\n",
      "\n",
      "Analyzing a single video on Linux/MacOS\n",
      "\n",
      ">>> deeplabcut.analyze_videos(\n",
      "        '/analysis/project/reaching-task/config.yaml',\n",
      "        ['/analysis/project/videos/reachingvideo1.avi'],\n",
      "    )\n",
      "\n",
      "Analyze all videos of type ``avi`` in a folder\n",
      "\n",
      ">>> deeplabcut.analyze_videos(\n",
      "        '/analysis/project/reaching-task/config.yaml',\n",
      "        ['/analysis/project/videos'],\n",
      "        videotype='.avi',\n",
      "    )\n",
      "\n",
      "Analyze multiple videos\n",
      "\n",
      ">>> deeplabcut.analyze_videos(\n",
      "        '/analysis/project/reaching-task/config.yaml',\n",
      "        [\n",
      "            '/analysis/project/videos/reachingvideo1.avi',\n",
      "            '/analysis/project/videos/reachingvideo2.avi',\n",
      "        ],\n",
      "    )\n",
      "\n",
      "Analyze multiple videos with ``shuffle=2``\n",
      "\n",
      ">>> deeplabcut.analyze_videos(\n",
      "        '/analysis/project/reaching-task/config.yaml',\n",
      "        [\n",
      "            '/analysis/project/videos/reachingvideo1.avi',\n",
      "            '/analysis/project/videos/reachingvideo2.avi',\n",
      "        ],\n",
      "        shuffle=2,\n",
      "    )\n",
      "\n",
      "Analyze multiple videos with ``shuffle=2``, save results as an additional csv file\n",
      "\n",
      ">>> deeplabcut.analyze_videos(\n",
      "        '/analysis/project/reaching-task/config.yaml',\n",
      "        [\n",
      "            '/analysis/project/videos/reachingvideo1.avi',\n",
      "            '/analysis/project/videos/reachingvideo2.avi',\n",
      "        ],\n",
      "        shuffle=2,\n",
      "        save_as_csv=True,\n",
      "    )\n",
      "\u001b[1;31mFile:\u001b[0m      c:\\users\\jalvarez\\appdata\\local\\anaconda3\\envs\\deeplabcut\\lib\\site-packages\\deeplabcut\\pose_estimation_tensorflow\\predict_videos.py\n",
      "\u001b[1;31mType:\u001b[0m      function"
     ]
    }
   ],
   "source": [
    "dlc.analyze_videos?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    deeplabcut.analyze_videos(\n",
    "        config_path,\n",
    "        [video],\n",
    "        auto_track=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** 'auto_track=True' will complete steps 10-11 for you automatically so you get the “final” H5 file. Use the below steps if you need to change the parameters of tracking based on your dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spatial and (locally) temporal grouping: Track body part assemblies frame-by-frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31mSignature:\u001b[0m\n",
      "\u001b[0mdlc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_detections2tracklets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mconfig\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mvideos\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mvideotype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mtrainingsetindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0moverwrite\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mdestfolder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mignore_bodyparts\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0minferencecfg\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mmodelprefix\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mgreedy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mcalibrate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mwindow_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0midentity_only\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mtrack_method\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mDocstring:\u001b[0m\n",
      "   This should be called at the end of deeplabcut.analyze_videos for multianimal projects!\n",
      "\n",
      "   Parameters\n",
      "   ----------\n",
      "   config : string\n",
      "       Full path of the config.yaml file as a string.\n",
      "\n",
      "   videos : list\n",
      "       A list of strings containing the full paths to videos for analysis or a path to the directory, where all the videos with same extension are stored.\n",
      "\n",
      "   videotype: string, optional\n",
      "       Checks for the extension of the video in case the input to the video is a directory.\n",
      "Only videos with this extension are analyzed.\n",
      "       If left unspecified, videos with common extensions ('avi', 'mp4', 'mov', 'mpeg', 'mkv') are kept.\n",
      "\n",
      "   shuffle: int, optional\n",
      "       An integer specifying the shuffle index of the training dataset used for training the network. The default is 1.\n",
      "\n",
      "   trainingsetindex: int, optional\n",
      "       Integer specifying which TrainingsetFraction to use. By default the first (note that TrainingFraction is a list in config.yaml).\n",
      "\n",
      "   overwrite: bool, optional.\n",
      "       Overwrite tracks file i.e. recompute tracks from full detections and overwrite.\n",
      "\n",
      "   destfolder: string, optional\n",
      "       Specifies the destination folder for analysis data (default is the path of the video). Note that for subsequent analysis this\n",
      "       folder also needs to be passed.\n",
      "\n",
      "   ignore_bodyparts: optional\n",
      "       List of body part names that should be ignored during tracking (advanced).\n",
      "       By default, all the body parts are used.\n",
      "\n",
      "   inferencecfg: Default is None.\n",
      "       Configuration file for inference (assembly of individuals). Ideally\n",
      "       should be obtained from cross validation (during evaluation). By default\n",
      "       the parameters are loaded from inference_cfg.yaml, but these get_level_values\n",
      "       can be overwritten.\n",
      "\n",
      "   calibrate: bool, optional (default=False)\n",
      "       If True, use training data to calibrate the animal assembly procedure.\n",
      "       This improves its robustness to wrong body part links,\n",
      "       but requires very little missing data.\n",
      "\n",
      "   window_size: int, optional (default=0)\n",
      "       Recurrent connections in the past `window_size` frames are\n",
      "       prioritized during assembly. By default, no temporal coherence cost\n",
      "       is added, and assembly is driven mainly by part affinity costs.\n",
      "\n",
      "   identity_only: bool, optional (default=False)\n",
      "       If True and animal identity was learned by the model,\n",
      "       assembly and tracking rely exclusively on identity prediction.\n",
      "\n",
      "   track_method: string, optional\n",
      "        Specifies the tracker used to generate the pose estimation data.\n",
      "        For multiple animals, must be either 'box', 'skeleton', or 'ellipse'\n",
      "        and will be taken from the config.yaml file if none is given.\n",
      "\n",
      "\n",
      "   Examples\n",
      "   --------\n",
      "   If you want to convert detections to tracklets:\n",
      "   >>> deeplabcut.convert_detections2tracklets('/analysis/project/reaching-task/config.yaml',[]'/analysis/project/video1.mp4'], videotype='.mp4')\n",
      "\n",
      "   If you want to convert detections to tracklets based on box_tracker:\n",
      "   >>> deeplabcut.convert_detections2tracklets('/analysis/project/reaching-task/config.yaml',[]'/analysis/project/video1.mp4'], videotype='.mp4',track_method='box')\n",
      "\n",
      "   --------\n",
      "\n",
      "   \n",
      "\u001b[1;31mFile:\u001b[0m      c:\\users\\jalvarez\\appdata\\local\\anaconda3\\envs\\deeplabcut\\lib\\site-packages\\deeplabcut\\pose_estimation_tensorflow\\predict_videos.py\n",
      "\u001b[1;31mType:\u001b[0m      function"
     ]
    }
   ],
   "source": [
    "dlc.convert_detections2tracklets?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    deeplabcut.convert_detections2tracklets(\n",
    "        config_path,\n",
    "        [video],\n",
    "        track_method=\"ellipse\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reconstruct full animal trajectories (tracks from tracklets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31mSignature:\u001b[0m\n",
      "\u001b[0mdlc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstitch_tracklets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mconfig_path\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mvideos\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mvideotype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mtrainingsetindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mn_tracks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mmin_length\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0msplit_tracklets\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mprestitch_residuals\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mmax_gap\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mweight_func\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mdestfolder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mmodelprefix\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mtrack_method\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0moutput_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mtransformer_checkpoint\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0msave_as_csv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mDocstring:\u001b[0m\n",
      "   Stitch sparse tracklets into full tracks via a graph-based,\n",
      "   minimum-cost flow optimization problem.\n",
      "\n",
      "   Parameters\n",
      "   ----------\n",
      "   config_path : str\n",
      "       Path to the main project config.yaml file.\n",
      "\n",
      "   videos : list\n",
      "       A list of strings containing the full paths to videos for analysis or a path to the directory, where all the videos with same extension are stored.\n",
      "\n",
      "   videotype: string, optional\n",
      "       Checks for the extension of the video in case the input to the video is a directory.\n",
      "Only videos with this extension are analyzed.\n",
      "       If left unspecified, videos with common extensions ('avi', 'mp4', 'mov', 'mpeg', 'mkv') are kept.\n",
      "\n",
      "   shuffle: int, optional\n",
      "       An integer specifying the shuffle index of the training dataset used for training the network. The default is 1.\n",
      "\n",
      "   trainingsetindex: int, optional\n",
      "       Integer specifying which TrainingsetFraction to use. By default the first (note that TrainingFraction is a list in config.yaml).\n",
      "\n",
      "   n_tracks : int, optional\n",
      "       Number of tracks to reconstruct. By default, taken as the number\n",
      "       of individuals defined in the config.yaml. Another number can be\n",
      "       passed if the number of animals in the video is different from\n",
      "       the number of animals the model was trained on.\n",
      "\n",
      "   min_length : int, optional\n",
      "       Tracklets less than `min_length` frames of length\n",
      "       are considered to be residuals; i.e., they do not participate\n",
      "       in building the graph and finding the solution to the\n",
      "       optimization problem, but are rather added last after\n",
      "       \"almost-complete\" tracks are formed. The higher the value,\n",
      "       the lesser the computational cost, but the higher the chance of\n",
      "       discarding relatively long and reliable tracklets that are\n",
      "       essential to solving the stitching task.\n",
      "       Default is 10, and must be 3 at least.\n",
      "\n",
      "   split_tracklets : bool, optional\n",
      "       By default, tracklets whose time indices are not consecutive integers\n",
      "       are split in shorter tracklets whose time continuity is guaranteed.\n",
      "       This is for example very powerful to get rid of tracking errors\n",
      "       (e.g., identity switches) which are often signaled by a missing\n",
      "       time frame at the moment they occur. Note though that for long\n",
      "       occlusions where tracker re-identification capability can be trusted,\n",
      "       setting `split_tracklets` to False is preferable.\n",
      "\n",
      "   prestitch_residuals : bool, optional\n",
      "       Residuals will by default be grouped together according to their\n",
      "       temporal proximity prior to being added back to the tracks.\n",
      "       This is done to improve robustness and simultaneously reduce complexity.\n",
      "\n",
      "   max_gap : int, optional\n",
      "       Maximal temporal gap to allow between a pair of tracklets.\n",
      "       This is automatically determined by the TrackletStitcher by default.\n",
      "\n",
      "   weight_func : callable, optional\n",
      "       Function accepting two tracklets as arguments and returning a scalar\n",
      "       that must be inversely proportional to the likelihood that the tracklets\n",
      "       belong to the same track; i.e., the higher the confidence that the\n",
      "       tracklets should be stitched together, the lower the returned value.\n",
      "\n",
      "   destfolder: string, optional\n",
      "       Specifies the destination folder for analysis data (default is the path of the video). Note that for subsequent analysis this\n",
      "       folder also needs to be passed.\n",
      "\n",
      "   track_method: string, optional\n",
      "        Specifies the tracker used to generate the pose estimation data.\n",
      "        For multiple animals, must be either 'box', 'skeleton', or 'ellipse'\n",
      "        and will be taken from the config.yaml file if none is given.\n",
      "\n",
      "   output_name : str, optional\n",
      "       Name of the output h5 file.\n",
      "       By default, tracks are automatically stored into the same directory\n",
      "       as the pickle file and with its name.\n",
      "\n",
      "   save_as_csv: bool, optional\n",
      "       Whether to write the tracks to a CSV file too (False by default).\n",
      "\n",
      "   Returns\n",
      "   -------\n",
      "   A TrackletStitcher object\n",
      "   \n",
      "\u001b[1;31mFile:\u001b[0m      c:\\users\\jalvarez\\appdata\\local\\anaconda3\\envs\\deeplabcut\\lib\\site-packages\\deeplabcut\\refine_training_dataset\\stitch.py\n",
      "\u001b[1;31mType:\u001b[0m      function"
     ]
    }
   ],
   "source": [
    "dlc.stitch_tracklets?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    deeplabcut.stitch_tracklets(\n",
    "    config_path,\n",
    "    [video],\n",
    "    track_method=\"ellipse\",\n",
    "    min_length=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a pretty video output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31mSignature:\u001b[0m\n",
      "\u001b[0mdlc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate_labeled_video\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mconfig\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mvideos\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mvideotype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mtrainingsetindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mfiltered\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mfastmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0msave_frames\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mkeypoints_only\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mFrames2plot\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mdisplayedbodyparts\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'all'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mdisplayedindividuals\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'all'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mcodec\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'mp4v'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0moutputframerate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mdestfolder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mdraw_skeleton\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mtrailpoints\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mdisplaycropped\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mcolor_by\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'bodypart'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mmodelprefix\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0minit_weights\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mtrack_method\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0msuperanimal_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mpcutoff\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.6\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mskeleton\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mskeleton_color\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'white'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mdotsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mcolormap\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'rainbow'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0malphavalue\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0moverwrite\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mconfidence_to_alpha\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCallable\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mDocstring:\u001b[0m\n",
      "Labels the bodyparts in a video.\n",
      "\n",
      "Make sure the video is already analyzed by the function\n",
      "``deeplabcut.analyze_videos``.\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "config : string\n",
      "    Full path of the config.yaml file.\n",
      "\n",
      "videos : list[str]\n",
      "    A list of strings containing the full paths to videos for analysis or a path\n",
      "    to the directory, where all the videos with same extension are stored.\n",
      "\n",
      "videotype: str, optional, default=\"\"\n",
      "    Checks for the extension of the video in case the input to the video is a\n",
      "    directory. Only videos with this extension are analyzed.\n",
      "    If left unspecified, videos with common extensions\n",
      "    ('avi', 'mp4', 'mov', 'mpeg', 'mkv') are kept.\n",
      "\n",
      "shuffle : int, optional, default=1\n",
      "    Number of shuffles of training dataset.\n",
      "\n",
      "trainingsetindex: int, optional, default=0\n",
      "    Integer specifying which TrainingsetFraction to use.\n",
      "    Note that TrainingFraction is a list in config.yaml.\n",
      "\n",
      "filtered: bool, optional, default=False\n",
      "    Boolean variable indicating if filtered output should be plotted rather than\n",
      "    frame-by-frame predictions. Filtered version can be calculated with\n",
      "    ``deeplabcut.filterpredictions``.\n",
      "\n",
      "fastmode: bool, optional, default=True\n",
      "    If ``True``, uses openCV (much faster but less customization of video) instead\n",
      "    of matplotlib if ``False``. You can also \"save_frames\" individually or not in\n",
      "    the matplotlib mode (if you set the \"save_frames\" variable accordingly).\n",
      "    However, using matplotlib to create the frames it therefore allows much more\n",
      "    flexible (one can set transparency of markers, crop, and easily customize).\n",
      "\n",
      "save_frames: bool, optional, default=False\n",
      "    If ``True``, creates each frame individual and then combines into a video.\n",
      "    Setting this to ``True`` is relatively slow as it stores all individual frames.\n",
      "\n",
      "keypoints_only: bool, optional, default=False\n",
      "    By default, both video frames and keypoints are visible. If ``True``, only the\n",
      "    keypoints are shown. These clips are an hommage to Johansson movies,\n",
      "    see https://www.youtube.com/watch?v=1F5ICP9SYLU and of course his seminal\n",
      "    paper: \"Visual perception of biological motion and a model for its analysis\"\n",
      "    by Gunnar Johansson in Perception & Psychophysics 1973.\n",
      "\n",
      "Frames2plot: List[int] or None, optional, default=None\n",
      "    If not ``None`` and ``save_frames=True`` then the frames corresponding to the\n",
      "    index will be plotted. For example, ``Frames2plot=[0,11]`` will plot the first\n",
      "    and the 12th frame.\n",
      "\n",
      "displayedbodyparts: list[str] or str, optional, default=\"all\"\n",
      "    This selects the body parts that are plotted in the video. If ``all``, then all\n",
      "    body parts from config.yaml are used. If a list of strings that are a subset of\n",
      "    the full list. E.g. ['hand','Joystick'] for the demo\n",
      "    Reaching-Mackenzie-2018-08-30/config.yaml to select only these body parts.\n",
      "\n",
      "displayedindividuals: list[str] or str, optional, default=\"all\"\n",
      "    Individuals plotted in the video.\n",
      "    By default, all individuals present in the config will be showed.\n",
      "\n",
      "codec: str, optional, default=\"mp4v\"\n",
      "    Codec for labeled video. For available options, see\n",
      "    http://www.fourcc.org/codecs.php. Note that this depends on your ffmpeg\n",
      "    installation.\n",
      "\n",
      "outputframerate: int or None, optional, default=None\n",
      "    Positive number, output frame rate for labeled video (only available for the\n",
      "    mode with saving frames.) If ``None``, which results in the original video\n",
      "    rate.\n",
      "\n",
      "destfolder: string or None, optional, default=None\n",
      "    Specifies the destination folder that was used for storing analysis data. If\n",
      "    ``None``, the path of the video file is used.\n",
      "\n",
      "draw_skeleton: bool, optional, default=False\n",
      "    If ``True`` adds a line connecting the body parts making a skeleton on each\n",
      "    frame. The body parts to be connected and the color of these connecting lines\n",
      "    are specified in the config file.\n",
      "\n",
      "trailpoints: int, optional, default=0\n",
      "    Number of previous frames whose body parts are plotted in a frame\n",
      "    (for displaying history).\n",
      "\n",
      "displaycropped: bool, optional, default=False\n",
      "    Specifies whether only cropped frame is displayed (with labels analyzed\n",
      "    therein), or the original frame with the labels analyzed in the cropped subset.\n",
      "\n",
      "color_by : string, optional, default='bodypart'\n",
      "    Coloring rule. By default, each bodypart is colored differently.\n",
      "    If set to 'individual', points belonging to a single individual are colored the\n",
      "    same.\n",
      "\n",
      "modelprefix: str, optional, default=\"\"\n",
      "    Directory containing the deeplabcut models to use when evaluating the network.\n",
      "    By default, the models are assumed to exist in the project folder.\n",
      "\n",
      "init_weights: str,\n",
      "    Checkpoint path to the super model\n",
      "\n",
      "track_method: string, optional, default=\"\"\n",
      "    Specifies the tracker used to generate the data.\n",
      "    Empty by default (corresponding to a single animal project).\n",
      "    For multiple animals, must be either 'box', 'skeleton', or 'ellipse' and will\n",
      "    be taken from the config.yaml file if none is given.\n",
      "\n",
      "overwrite: bool, optional, default=False\n",
      "    If ``True`` overwrites existing labeled videos.\n",
      "\n",
      "confidence_to_alpha: Union[bool, Callable[[float], float], default=False\n",
      "    If False, all keypoints will be plot with alpha=1. Otherwise, this can be\n",
      "    defined as a function f: [0, 1] -> [0, 1] such that the alpha value for a\n",
      "    keypoint will be set as a function of its score: alpha = f(score). The default\n",
      "    function used when True is f(x) = max(0, (x - pcutoff)/(1 - pcutoff)).\n",
      "\n",
      "Returns\n",
      "-------\n",
      "    results : list[bool]\n",
      "    ``True`` if the video is successfully created for each item in ``videos``.\n",
      "\n",
      "Examples\n",
      "--------\n",
      "\n",
      "Create the labeled video for a single video\n",
      "\n",
      ">>> deeplabcut.create_labeled_video(\n",
      "        '/analysis/project/reaching-task/config.yaml',\n",
      "        ['/analysis/project/videos/reachingvideo1.avi'],\n",
      "    )\n",
      "\n",
      "Create the labeled video for a single video and store the individual frames\n",
      "\n",
      ">>> deeplabcut.create_labeled_video(\n",
      "        '/analysis/project/reaching-task/config.yaml',\n",
      "        ['/analysis/project/videos/reachingvideo1.avi'],\n",
      "        fastmode=True,\n",
      "        save_frames=True,\n",
      "    )\n",
      "\n",
      "Create the labeled video for multiple videos\n",
      "\n",
      ">>> deeplabcut.create_labeled_video(\n",
      "        '/analysis/project/reaching-task/config.yaml',\n",
      "        [\n",
      "            '/analysis/project/videos/reachingvideo1.avi',\n",
      "            '/analysis/project/videos/reachingvideo2.avi',\n",
      "        ],\n",
      "    )\n",
      "\n",
      "Create the labeled video for all the videos with an .avi extension in a directory.\n",
      "\n",
      ">>> deeplabcut.create_labeled_video(\n",
      "        '/analysis/project/reaching-task/config.yaml',\n",
      "        ['/analysis/project/videos/'],\n",
      "    )\n",
      "\n",
      "Create the labeled video for all the videos with an .mp4 extension in a directory.\n",
      "\n",
      ">>> deeplabcut.create_labeled_video(\n",
      "        '/analysis/project/reaching-task/config.yaml',\n",
      "        ['/analysis/project/videos/'],\n",
      "        videotype='mp4',\n",
      "    )\n",
      "\u001b[1;31mFile:\u001b[0m      c:\\users\\jalvarez\\appdata\\local\\anaconda3\\envs\\deeplabcut\\lib\\site-packages\\deeplabcut\\utils\\make_labeled_video.py\n",
      "\u001b[1;31mType:\u001b[0m      function"
     ]
    }
   ],
   "source": [
    "dlc.create_labeled_video?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31mSignature:\u001b[0m\n",
      "\u001b[0mdlc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilterpredictions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mconfig\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mvideo\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mvideotype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mtrainingsetindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mfiltertype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'median'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mwindowlength\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mp_bound\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.001\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mARdegree\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mMAdegree\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0malpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.01\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0msave_as_csv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mdestfolder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mmodelprefix\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mtrack_method\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mreturn_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mDocstring:\u001b[0m\n",
      "Fits frame-by-frame pose predictions.\n",
      "\n",
      "The pose predictions are fitted with ARIMA model (filtertype='arima') or median\n",
      "filter (default).\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "config : string\n",
      "    Full path of the config.yaml file.\n",
      "\n",
      "video : string\n",
      "    Full path of the video to extract the frame from. Make sure that this video is\n",
      "    already analyzed.\n",
      "\n",
      "shuffle : int, optional, default=1\n",
      "    The shuffle index of training dataset. The extracted frames will be stored in\n",
      "    the labeled-dataset for the corresponding shuffle of training dataset.\n",
      "\n",
      "trainingsetindex: int, optional, default=0\n",
      "    Integer specifying which TrainingsetFraction to use.\n",
      "    Note that TrainingFraction is a list in config.yaml.\n",
      "\n",
      "filtertype: string, optional, default=\"median\".\n",
      "    The filter type - 'arima', 'median' or 'spline'.\n",
      "\n",
      "windowlength: int, optional, default=5\n",
      "    For filtertype='median' filters the input array using a local window-size given\n",
      "    by windowlength. The array will automatically be zero-padded.\n",
      "    https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.medfilt.html.\n",
      "    The windowlenght should be an odd number.\n",
      "    If filtertype='spline', windowlength is the maximal gap size to fill.\n",
      "\n",
      "p_bound: float between 0 and 1, optional, default=0.001\n",
      "    For filtertype 'arima' this parameter defines the likelihood below,\n",
      "    below which a body part will be consided as missing data for filtering purposes.\n",
      "\n",
      "ARdegree: int, optional, default=3\n",
      "    For filtertype 'arima' Autoregressive degree of Sarimax model degree.\n",
      "    see https://www.statsmodels.org/dev/generated/statsmodels.tsa.statespace.sarimax.SARIMAX.html\n",
      "\n",
      "MAdegree: int, optional, default=1\n",
      "    For filtertype 'arima' Moving Average degree of Sarimax model degree.\n",
      "    See https://www.statsmodels.org/dev/generated/statsmodels.tsa.statespace.sarimax.SARIMAX.html\n",
      "\n",
      "alpha: float, optional, default=0.01\n",
      "    Significance level for detecting outliers based on confidence interval of fitted SARIMAX model.\n",
      "\n",
      "save_as_csv: bool, optional, default=True\n",
      "    Saves the predictions in a .csv file.\n",
      "\n",
      "destfolder: string, optional, default=None\n",
      "    Specifies the destination folder for analysis data. If ``None``, the path of\n",
      "    the video is used by default. Note that for subsequent analysis this folder\n",
      "    also needs to be passed.\n",
      "\n",
      "modelprefix: str, optional, default=\"\"\n",
      "    Directory containing the deeplabcut models to use when evaluating the network.\n",
      "    By default, the models are assumed to exist in the project folder.\n",
      "\n",
      "track_method: string, optional, default=\"\"\n",
      "    Specifies the tracker used to generate the data.\n",
      "    Empty by default (corresponding to a single animal project).\n",
      "    For multiple animals, must be either 'box', 'skeleton', or 'ellipse' and will\n",
      "    be taken from the config.yaml file if none is given.\n",
      "\n",
      "return_data: bool, optional, default=False\n",
      "    If True, returns a dictionary of the filtered data keyed by video names.\n",
      "\n",
      "Returns\n",
      "-------\n",
      "video_to_filtered_df\n",
      "    Dictionary mapping video filepaths to filtered dataframes.\n",
      "\n",
      "    * If no videos exist, the dictionary will be empty.\n",
      "    * If a video is not analyzed, the corresponding value in the dictionary will be\n",
      "      None.\n",
      "\n",
      "Examples\n",
      "--------\n",
      "\n",
      "Arima model:\n",
      "\n",
      ">>> deeplabcut.filterpredictions(\n",
      "        'C:\\myproject\\reaching-task\\config.yaml',\n",
      "        ['C:\\myproject\\trailtracking-task\\test.mp4'],\n",
      "        shuffle=3,\n",
      "        filterype='arima',\n",
      "        ARdegree=5,\n",
      "        MAdegree=2,\n",
      "    )\n",
      "\n",
      "Use median filter over 10 bins:\n",
      "\n",
      ">>> deeplabcut.filterpredictions(\n",
      "        'C:\\myproject\\reaching-task\\config.yaml',\n",
      "        ['C:\\myproject\\trailtracking-task\\test.mp4'],\n",
      "        shuffle=3,\n",
      "        windowlength=10,\n",
      "    )\n",
      "\n",
      "One can then use the filtered rather than the frame-by-frame predictions by calling:\n",
      "\n",
      ">>> deeplabcut.plot_trajectories(\n",
      "        'C:\\myproject\\reaching-task\\config.yaml',\n",
      "        ['C:\\myproject\\trailtracking-task\\test.mp4'],\n",
      "        shuffle=3,\n",
      "        filtered=True,\n",
      "    )\n",
      "\n",
      ">>> deeplabcut.create_labeled_video(\n",
      "        'C:\\myproject\\reaching-task\\config.yaml',\n",
      "        ['C:\\myproject\\trailtracking-task\\test.mp4'],\n",
      "        shuffle=3,\n",
      "        filtered=True,\n",
      "    )\n",
      "\u001b[1;31mFile:\u001b[0m      c:\\users\\jalvarez\\appdata\\local\\anaconda3\\envs\\deeplabcut\\lib\\site-packages\\deeplabcut\\post_processing\\filtering.py\n",
      "\u001b[1;31mType:\u001b[0m      function"
     ]
    }
   ],
   "source": [
    "dlc.filterpredictions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filter the predictions to remove small jitter, if desired:\n",
    "dlc.filterpredictions(config_path, \n",
    "                                 [video], \n",
    "                                 shuffle=0,\n",
    "                                 videotype='mp4', \n",
    "                                 track_method = TRACK_METHOD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    deeplabcut.create_labeled_video(\n",
    "        config_path,\n",
    "        [video],\n",
    "        color_by=\"individual\",\n",
    "        keypoints_only=False,\n",
    "        trailpoints=10,\n",
    "        draw_skeleton=False,\n",
    "        track_method=\"ellipse\",\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DEEPLABCUT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
